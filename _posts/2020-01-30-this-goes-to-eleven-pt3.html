<p>Since there’s a lot to go over here, I’ve split it up into a few parts:</p>

<ol>
  <li>In <a href="/2020-01-28/this-goes-to-eleven-pt1">part 1</a>, we start with a refresher on <code class="highlighter-rouge">QuickSort</code> and how it compares to <code class="highlighter-rouge">Array.Sort()</code>.</li>
  <li>In <a href="/2020-01-29/this-goes-to-eleven-pt2">part 2</a>, we go over the basics of vectorized hardware intrinsics, vector types, and go over a handful of vectorized instructions we’ll use in part 3. We still won’t be sorting anything.</li>
  <li>In this part, we go through the initial code for the vectorized sorting, and start seeing some payoff. We finish agonizing courtesy of the CPU’s branch predictor, throwing a wrench into our attempts.</li>
  <li>In part 4, we go over a handful of optimization approaches that I attempted trying to get the vectorized partitioning to run faster. We’ll see what worked and what didn’t.</li>
  <li>In part 5, we’ll see how we can almost get rid of all the remaining scalar code- by implementing small-constant size array sorting. We’ll use, drum roll…, yet more AVX2 vectorization.</li>
  <li>Finally, in part 6, I’ll list the outstanding stuff/ideas I have for getting more juice and functionality out of my vectorized code.</li>
</ol>

<h2 id="unstable-vectorized-partitioning--quicksort">Unstable Vectorized Partitioning + QuickSort</h2>

<p>It’s time we mash all the new knowledge we picked up in the last posts about SIMD registers, instructions, and <code class="highlighter-rouge">QuickSort</code>ing into something useful. Here’s the plan:</p>

<ul>
  <li>Vectorized in-place partitioning:
    <ul>
      <li>First, we learn to take 8-element blocks, or units of <code class="highlighter-rouge">Vector256&lt;int&gt;</code>, and partition them with AVX2 intrinsics.</li>
      <li>Then we take Berlin: We reuse our block to partition an entire array with a method I named double-pumping, suitable for processing large arrays in-place with this vectorized block.</li>
    </ul>
  </li>
  <li>Once we’ve covered vectorized partitioning, we finish up with some innocent glue-code wrapping the whole thing to look like a proper <code class="highlighter-rouge">Array.Sort</code> replacement.</li>
</ul>

<p>Now that we’re doing our own thing, finally, It’s time to address a baby elephant hiding in the room: Stable vs. Unstable sorting. I should probably bother explaining: One possible way to categorize sorting algorithms is with respect for their stability: Do they reorder <em>equal</em> values as they appear in the original input data or not. Stable sorting does not reorder, while unstable sorting provides no such guarantee.<br />
Stability <em>might</em> be critical for certain tasks, for example:</p>

<ul>
  <li>When sorting an array of structs/classes according to a key embedded as a member, while providing a non-default <code class="highlighter-rouge">IComparer&lt;T&gt;</code> or <code class="highlighter-rouge">Comparison&lt;T&gt;</code>, we might care about preserving the order of the containing type.</li>
  <li>Similarly, when sorting pairs of arrays: keys and values, reordering both arrays according to the sorted order of the keys, while preserving the ordering of values for equal keys.</li>
</ul>

<p>At the same time, stable sorting is a non-issue when:</p>

<ul>
  <li>Sorting arrays of simple primitives; stability is meaningless:<br />
(what would a “stable sort” of the array <code class="highlighter-rouge">[7, 7, 7]</code> even mean?)</li>
  <li>At other times, we <em>know</em> for a fact that our keys are unique. There is no unstable sorting for unique keys.</li>
  <li>Lastly, sometimes, <em>we just don’t care</em>. We’re fine if our data gets reordered.</li>
</ul>

<p>In the .NET/C# world, one could say that the landscape regarding sorting is a little unstable (pun intended):</p>

<ul>
  <li>
    <p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.array.sort?view=netcore-3.1"><code class="highlighter-rouge">Array.Sort</code></a> is unstable, as is clearly stated in the remarks section:</p>

    <blockquote>
      <p>This implementation performs an unstable sort; that is, if two elements are equal, their order might not be preserved.</p>
    </blockquote>
  </li>
  <li>
    <p>On the other hand, <a href="https://docs.microsoft.com/en-us/dotnet/api/system.linq.enumerable.orderby?view=netcore-3.1"><code class="highlighter-rouge">Enumerable.OrderBy</code></a> is stable:</p>

    <blockquote>
      <p>This method performs a stable sort; that is, if the keys of two elements are equal, the order of the elements is preserved.</p>
    </blockquote>
  </li>
</ul>

<p>In general, what I came up with in my full repo/nuget package are algorithms capable of doing both stable and unstable sorting. But with two caveats:</p>

<ul>
  <li>Stable sorting is considerably slower than unstable sorting (But still faster than <code class="highlighter-rouge">Array.Sort</code>).</li>
  <li>Stable sorting is less elegant/fun to explain.</li>
</ul>

<p>Given this new information and the fact that I am only presenting pure primitive sorting anyway, where there is no notion of stability to begin with, for this series, I will be describing my unstable sorting approach. It doesn’t take a lot of imagination to get from here to the stable variant, but I’m not going to address this in these posts. It is also important to note that in general, when there is a doubt if stability is a requirement (e.g., for key/value, <code class="highlighter-rouge">IComparer&lt;T&gt;</code>/<code class="highlighter-rouge">Comparison&lt;T&gt;</code>, or non-primitive sorting) we should err on the side of safety and go for stable sorting.</p>

<h3 id="avx2-partitioning-block">AVX2 Partitioning Block</h3>

<p>Let’s start with this “simple” block, describing what we do with moving pictures.</p>

<table style="margin-bottom: 0em" class="notice--info">
<tr>
<td style="border: none; padding-top: 0; padding-bottom: 0; vertical-align: top"><span class="uk-label">Hint</span></td>
<td style="border: none; padding-top: 0; padding-bottom: 0">From here-on, The following icon means I have a thingy that animates:
<object style="margin: auto; vertical-align: middle;" type="image/svg+xml" data="../talks/intrinsics-sorting-2019/play.svg"></object><br />
Click/Touch/Hover <b>inside</b> means: <i class="glyphicon glyphicon-play"></i><br />
Click/Touch/Hover <b>outside</b> means: <i class="glyphicon glyphicon-pause"></i>
</td>
</tr>
</table>

<object class="animated-border" width="100%" type="image/svg+xml" data="../talks/intrinsics-sorting-2019/block-unified-with-hint.svg"></object>
<p>Here is the same block, in more traditional code form:</p>

<div>
  <div class="stickemup">

    <div class="language-csharp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td class="rouge-code"><pre><span class="kt">var</span> <span class="n">P</span> <span class="p">=</span> <span class="n">Vector256</span><span class="p">.</span><span class="nf">Create</span><span class="p">(</span><span class="n">pivot</span><span class="p">);</span> <span class="c1">// Outside any loop, top-level in the function</span>
<span class="p">...</span>
<span class="p">[</span><span class="nf">MethodImpl</span><span class="p">(</span><span class="n">MethodImplOptions</span><span class="p">.</span><span class="n">AggressiveInlining</span><span class="p">)]</span>
<span class="k">static</span> <span class="k">unsafe</span> <span class="k">void</span> <span class="nf">PartitionBlock</span><span class="p">(</span><span class="kt">int</span> <span class="p">*</span><span class="n">dataPtr</span><span class="p">,</span> <span class="n">Vector256</span><span class="p">&lt;</span><span class="kt">int</span><span class="p">&gt;</span> <span class="n">P</span><span class="p">,</span>
                                  <span class="k">ref</span> <span class="kt">int</span><span class="p">*</span> <span class="n">writeLeft</span><span class="p">,</span> <span class="k">ref</span> <span class="kt">int</span><span class="p">*</span> <span class="n">writeRight</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">var</span> <span class="n">data</span> <span class="p">=</span> <span class="n">Avx2</span><span class="p">.</span><span class="nf">LoadDquVector256</span><span class="p">(</span><span class="n">dataPtr</span><span class="p">);</span>
    <span class="kt">var</span> <span class="n">mask</span> <span class="p">=</span> <span class="p">(</span><span class="kt">uint</span><span class="p">)</span> <span class="n">Avx</span><span class="p">.</span><span class="nf">MoveMask</span><span class="p">(</span>
        <span class="n">Avx2</span><span class="p">.</span><span class="nf">CompareGreaterThan</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">P</span><span class="p">).</span><span class="nf">AsSingle</span><span class="p">());</span>
    <span class="n">data</span> <span class="p">=</span> <span class="n">Avx2</span><span class="p">.</span><span class="nf">PermuteVar8x32</span><span class="p">(</span><span class="n">data</span><span class="p">,</span>
        <span class="n">Avx2</span><span class="p">.</span><span class="nf">LoadDquVector256</span><span class="p">(</span><span class="n">PermTablePtr</span> <span class="p">+</span> <span class="n">mask</span> <span class="p">*</span> <span class="m">8</span><span class="p">)));</span>
    <span class="n">Avx</span><span class="p">.</span><span class="nf">Store</span><span class="p">(</span><span class="n">writeLeft</span><span class="p">,</span>  <span class="n">data</span><span class="p">);</span>
    <span class="n">Avx</span><span class="p">.</span><span class="nf">Store</span><span class="p">(</span><span class="n">writeRight</span><span class="p">,</span> <span class="n">data</span><span class="p">);</span>
    <span class="kt">var</span> <span class="n">popCount</span> <span class="p">=</span> <span class="n">PopCnt</span><span class="p">.</span><span class="nf">PopCount</span><span class="p">(</span><span class="n">mask</span><span class="p">);</span>
    <span class="n">writeRight</span> <span class="p">-=</span> <span class="n">pc</span><span class="p">;</span>
    <span class="n">writeLeft</span>  <span class="p">+=</span> <span class="m">8</span> <span class="p">-</span> <span class="n">pc</span><span class="p">;</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>

  </div>

  <p>There’s a lot of cheese here; let’s break this down:</p>

  <div class="divTable">
<div class="divTableBody">
<div class="divTableRow">
<div class="divTableCell"><span class="uk-label">L1</span></div>
<div class="divTableCell">
          <p>Broadcast the pivot value to a vector I’ve named <code class="highlighter-rouge">P</code>. We’re merely creating 8-copies of the selected pivot value in a SIMD register.<br />
Technically, this isn’t really part of the block as this is this happens only <em>once</em> per partitioning function call! It’s included here for context.</p>
        </div>
</div>
<div class="divTableRow">
<div class="divTableCell"><span class="uk-label">L3-5</span></div>
<div class="divTableCell">
          <p>We wrap our block in a static function. We aggressively inline it in strategic places throughout the rest of the code.<br />
This may look like an odd signature, but think of its purpose: We avoid copy-pasting codemwhile also avoiding any performance penalty.</p>
        </div>
</div>
<div class="divTableRow">
<div class="divTableCell"><span class="uk-label">L6</span></div>
<div class="divTableCell">
          <p>Load up data from somewhere in our array. <code class="highlighter-rouge">dataPtr</code> points to some unpartitioned data. <code class="highlighter-rouge">dataVec</code> will be loaded with data we intend to partition, and that’s the important bit.</p>
        </div>
</div>
<div class="divTableRow">
<div class="divTableCell"><span class="uk-label">L7-8</span></div>
<div class="divTableCell">
          <p>Perform an 8-way comparison using <code class="highlighter-rouge">CompareGreaterThan</code>, then proceed to convert/compress the 256-bit result into an 8-bit value using the <code class="highlighter-rouge">MoveMask</code> intrinsic.<br />
The goal here is to generate a <strong>scalar</strong> <code class="highlighter-rouge">mask</code> value, that contains a single <code class="highlighter-rouge">1</code> bit for every comparison where the corresponding data element was <em>greater-than</em> the pivot value and <code class="highlighter-rouge">0</code> bits for all others. If you are having a hard time following <em>why</em> this does this, you need to head back to the <a href="/2020-01-29/this-goes-to-eleven-pt2">2<sup>nd</sup> post</a> and read up on these two intrinsics/watch their animations.</p>
        </div>
</div>
<div class="divTableRow">
<div class="divTableCell"><span class="uk-label">L9-10</span></div>
<div class="divTableCell">
          <p>Permute the loaded data according to a permutation vector; A-ha! A twist in the plot!<br />
<code class="highlighter-rouge">mask</code> contains 8 bits, from LSB to MSB describing where each element belongs to (left/right). We could, of course, loop over those bits and perform 8 branches to determine which side each element belongs to, but that would be a terrible mistake. Instead, we’re going to use the <code class="highlighter-rouge">mask</code> as an <em>index</em> into a lookup-table for permutation values!<br />
This is one of the reasons it was critical to use <code class="highlighter-rouge">MoveMask</code> in the first place. Without it, we would not have a scalar value we could use as an index to our table. Pretty neat, no?<br />
With the permutation operation done, we’ve grouped all the <em>smaller-or-equal</em> than values on one side of our <code class="highlighter-rouge">dataVec</code> vector (the “left” side) and all the <em>greater-than</em> values on the other side (the “right” side).<br />
I’ve comfortably glanced over the actual values in the permutation lookup-table which <code class="highlighter-rouge">PermTablePtr</code> is pointing to; I’ll address this a couple of paragraphs below.</p>
        </div>
</div>
</div>
</div>

  <p>Partitioning is now practically complete: That is, our <code class="highlighter-rouge">dataVec</code> vector is neatly partitioned. Except that that data is still “stuck” inside our vector. We need to write its contents back to memory. Here comes a small complication: Our <code class="highlighter-rouge">dataVec</code> vector now contains values belonging <em>both</em> to the left and right sides of the original array. We did separate them <strong>within</strong> the vector, but we’re not done until each side is written back to memory, on both ends of our array.</p>

  <div class="divTable">
<div class="divTableBody">
<div class="divTableRow">
<div class="divTableCell"><span class="uk-label">L11-12</span></div>
<div class="divTableCell">
          <p>Store the permuted vector to both sides of the array. There is no cheap way to write <em>portions</em> of a vector to each respective end, so we write the <strong>entire</strong> partitioned vector to both the <em>left</em> <strong>and</strong> <em>right</em> sides of the array.<br />
At any given moment, we have two write pointers pointing to where we need to write to <strong>next</strong> on either side: <code class="highlighter-rouge">writeLeft</code> and <code class="highlighter-rouge">writeRight</code>. How those are initialized and maintained will be dealt with further down where we start calling this block, but for now, let’s assume these pointers initially point to somewhere where it is <strong>safe</strong> to write <em>at least</em> an entire <code class="highlighter-rouge">Vector256&lt;T&gt;</code> and move on.</p>
        </div>
</div>
<div class="divTableRow">
<div class="divTableCell"><span class="uk-label">L13-15</span></div>
<div class="divTableCell">
          <p>Book-keeping time: We just wrote 8 elements to each side, and each side had a trail of unwanted data tacked to it. We didn’t care for it while we were writing it, because we knew we’re about to update the same write pointers in such a way that the <em>next</em> writes operations will <strong>overwrite</strong> the trailing/unwanted data that doesn’t belong to each respective side!<br />
The vector gods are smiling at us: We have the <code class="highlighter-rouge">PopCount</code> intrinsic to lend us a hand here. We issue <code class="highlighter-rouge">PopCount</code> on the same <code class="highlighter-rouge">mask</code> variable (again, <code class="highlighter-rouge">MoveMask</code> was worth its weight in gold here) and get a count of how many bits in <code class="highlighter-rouge">mask</code> were <code class="highlighter-rouge">1</code>. This accounts for how many values <strong>inside</strong> the vector were <em>greater-than</em> the pivot value and belong to the right side.<br />
This “happens” to be the amount by which we want to <em>decrease</em> the <code class="highlighter-rouge">writeRight</code> pointer (<code class="highlighter-rouge">writeRight</code> is “advanced” by decrementing it, this may seem weird for now, but will become clearer when we discuss the outer-loop!<br />
Finally, we adjust the <code class="highlighter-rouge">writeLeft</code> pointer: <code class="highlighter-rouge">popCount</code> contains the number of <code class="highlighter-rouge">1</code> bits; the number of <code class="highlighter-rouge">0</code> bits is by definition, <code class="highlighter-rouge">8 - popCount</code> since <code class="highlighter-rouge">mask</code> had 8 bits of content in it, to begin with. This accounts for how many values in the register were <em>less-than-or-equal</em> the pivot value and grouped on the left side of the register.</p>
        </div>
</div>
</div>
</div>

  <p>This was a full 8-element wise partitioning block, and it’s worth noting a thing or two about it:</p>

  <ul>
    <li>It is completely branch-less(!): We’ve given the CPU a nice juicy block with no need to speculate on what code gets executed next. It sure looks pretty when you consider the number of branches our scalar code would execute for the same amount of work. Don’t pop a champagne bottle quite yet though, we’re about to run into a wall full of thorny branches in a second, but sure feels good for now.</li>
    <li>If we want to execute multiple copies of this block, the main dependency from one block to the next is the mutation of the <code class="highlighter-rouge">writeLeft</code> and <code class="highlighter-rouge">writeRight</code> pointers. It’s unavoidable given we set-out to perform in-place sorting (well, I couldn’t avoid it, maybe you can!), but worth-while mentioning nonetheless. If you need a reminder about how these data-dependencies can change the dynamics of efficient execution, you can read up on when I tried my best to go at it battling with <a href="/2018-08-20/netcoreapp3.0-intrinsics-in-real-life-pt3"><code class="highlighter-rouge">PopCount</code> to run screaming fast</a>; If nothing else, you’ll get a clearer understanding of how the CPU extracts data-flows from our code.</li>
  </ul>

  <p>I thought it would be nice to wrap up the discussion of this block by showing off that the JIT is relatively well-behaved in this case with the generated x64 asm:<br />
Anyone who has followed the C# code can use the intrinsics table from the previous post and read the assembly code without further help. Also, it becomes clear how this is a 1:1 translation of C# code. Congratulations: It’s 2020, and we’re x86 assembly programmers again!</p>
</div>

<div class="language-nasm highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="rouge-code"><pre><span class="nf">vmovd</span> <span class="nv">xmm1</span><span class="p">,</span><span class="nb">r15d</span>                      <span class="c1">; Broadcast</span>
<span class="nf">vbroadcastd</span> <span class="nv">ymm1</span><span class="p">,</span><span class="nv">xmm1</span>                <span class="c1">; pivot</span>
<span class="nf">...</span>
<span class="nf">vlddqu</span> <span class="nv">ymm0</span><span class="p">,</span> <span class="nv">ymmword</span> <span class="nv">ptr</span> <span class="p">[</span><span class="nb">rax</span><span class="p">]</span>       <span class="c1">; load 8 elements</span>
<span class="nf">vpcmpgtd</span> <span class="nv">ymm2</span><span class="p">,</span> <span class="nv">ymm0</span><span class="p">,</span> <span class="nv">ymm1</span>            <span class="c1">; compare</span>
<span class="nf">vmovmskps</span> <span class="nb">ecx</span><span class="p">,</span> <span class="nv">ymm2</span>                  <span class="c1">; movemask into scalar reg</span>
<span class="nf">mov</span> <span class="nb">r9d</span><span class="p">,</span> <span class="nb">ecx</span>                         <span class="c1">; copy to r9</span>
<span class="nf">shl</span> <span class="nb">r9d</span><span class="p">,</span> <span class="mh">0x3</span>                         <span class="c1">; *= 8</span>
<span class="nf">vlddqu</span> <span class="nv">ymm2</span><span class="p">,</span> <span class="kt">qword</span> <span class="nv">ptr</span> <span class="p">[</span><span class="nb">rdx</span><span class="o">+</span><span class="nb">r9d</span><span class="o">*</span><span class="mi">4</span><span class="p">]</span>   <span class="c1">; load permutation</span>
<span class="nf">vpermd</span> <span class="nv">ymm0</span><span class="p">,</span> <span class="nv">ymm2</span><span class="p">,</span> <span class="nv">ymm0</span>              <span class="c1">; permute</span>
<span class="nf">vmovdqu</span> <span class="nv">ymmword</span> <span class="nv">ptr</span> <span class="p">[</span><span class="nv">r12</span><span class="p">],</span> <span class="nv">ymm0</span>      <span class="c1">; store left</span>
<span class="nf">vmovdqu</span> <span class="nv">ymmword</span> <span class="nv">ptr</span> <span class="p">[</span><span class="nv">r8</span><span class="p">],</span> <span class="nv">ymm0</span>       <span class="c1">; store right</span>
<span class="nf">popcnt</span> <span class="nb">ecx</span><span class="p">,</span> <span class="nb">ecx</span>                      <span class="c1">; popcnt</span>
<span class="nf">...</span>                                  <span class="c1">; update writeLeft/writeRight pointers</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="permutation-lookup-table">Permutation lookup table</h2>

<p>If you made it this far, you are owed an explanation of the permutation lookup table. Let’s see what’s in it:</p>

<ul>
  <li>The table needs to have 2<sup>8</sup> elements for all possible mask values.</li>
  <li>Each element ultimately needs to be a <code class="highlighter-rouge">Vector256&lt;int&gt;</code> because that’s what the permutation intrinsic expects from us, so 8 x 4 bytes = 32 bytes per element.
    <ul>
      <li>That’s a whopping 8kb of lookup data in total (!).</li>
    </ul>
  </li>
  <li>The values inside are <a href="https://github.com/damageboy/VxSort/blob/research/TestBlog/PermutationTableTests.cs#L20">pre-generated</a> so that they would reorder the data <em>inside</em> a <code class="highlighter-rouge">Vector256&lt;int&gt;</code> according to our wishes: all values that got a corresponding <code class="highlighter-rouge">1</code> bit in the mask go to one side (right side), and the elements with a <code class="highlighter-rouge">0</code> go to the other side (left side). There’s no particular required order amongst the grouped elements since we’re merely partitioning around a pivot value, nothing more, nothing less.</li>
</ul>

<p>Here are 4 sample values from the generated permutation table that I’ve copy-pasted so we can get a feel for it:</p>

<div>
  <div class="stickemup">

    <div class="language-csharp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="k">static</span> <span class="n">ReadOnlySpan</span><span class="p">&lt;</span><span class="kt">int</span><span class="p">&gt;</span> <span class="n">PermTable</span> <span class="p">=&gt;</span> <span class="k">new</span><span class="p">[]</span> <span class="p">{</span>
    <span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">3</span><span class="p">,</span> <span class="m">4</span><span class="p">,</span> <span class="m">5</span><span class="p">,</span> <span class="m">6</span><span class="p">,</span> <span class="m">7</span><span class="p">,</span>     <span class="c1">// 0   =&gt; 0b00000000</span>
    <span class="c1">// ...</span>
    <span class="m">3</span><span class="p">,</span> <span class="m">4</span><span class="p">,</span> <span class="m">5</span><span class="p">,</span> <span class="m">6</span><span class="p">,</span> <span class="m">7</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span>     <span class="c1">// 7   =&gt; 0b00000111</span>
    <span class="c1">// ...</span>
    <span class="m">0</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">4</span><span class="p">,</span> <span class="m">6</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">3</span><span class="p">,</span> <span class="m">5</span><span class="p">,</span> <span class="m">7</span><span class="p">,</span>     <span class="c1">// 170 =&gt; 0b10101010</span>
    <span class="c1">// ...</span>
    <span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">3</span><span class="p">,</span> <span class="m">4</span><span class="p">,</span> <span class="m">5</span><span class="p">,</span> <span class="m">6</span><span class="p">,</span> <span class="m">7</span><span class="p">,</span>     <span class="c1">// 255 =&gt; 0b11111111</span>
<span class="p">};</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>

  </div>

  <ul>
    <li>For <code class="highlighter-rouge">mask</code> values 0, 255 the entries are trivial: All <code class="highlighter-rouge">mask</code> bits were either <code class="highlighter-rouge">1</code> or <code class="highlighter-rouge">0</code> so there’s nothing we need to do with the data, we just leave it as is, the “null” permutation vector: <code class="highlighter-rouge">[0, 1, 2, 3, 4, 5, 6, 7]</code> achieves just that.</li>
    <li>When <code class="highlighter-rouge">mask</code> is <code class="highlighter-rouge">0b00000111</code> (decimal 7), the 3 lowest bits of the <code class="highlighter-rouge">mask</code> are <code class="highlighter-rouge">1</code>, they represent elements that need to go to the right side of the vector (e.g., elements that were <code class="highlighter-rouge">&gt; pivot</code>), while all other values need to go to the left (<code class="highlighter-rouge">&lt;= pivot</code>). The permutation vector: <code class="highlighter-rouge">[3, 4, 5, 6, 7, 0, 1, 2]</code> does just that.</li>
    <li>The checkered bit pattern for the <code class="highlighter-rouge">mask</code> value <code class="highlighter-rouge">0b10101010</code> (decimal 170) calls to move all the even elements to one side and the odd elements to the other… You can see that <code class="highlighter-rouge">[0, 2, 4, 6, 1, 3, 5, 7]</code> does the work here.</li>
  </ul>

  <table style="margin-bottom: 0em" class="notice--warning">
<tr>
<td style="border: none; padding-top: 0; padding-bottom: 0; vertical-align: top"><span class="uk-label uk-label-warning">Note</span></td>
<td style="border: none; padding-top: 0; padding-bottom: 0"><div>
          <p>The permutation table signature provided here is technically a lie: The <a href="https://github.com/damageboy/VxSort/blob/research/VxSortResearch/PermutationTables/Int32PermTables.cs#L12">actual code</a> uses <code class="highlighter-rouge">ReadOnlySpan&lt;byte&gt;</code> as the table’s type, with the <code class="highlighter-rouge">int</code> values encoded as individual bytes in little-endian encoding. This is a C# 7.3 specific optimization where we get to treat the address of this table as a constant at JIT time. Kevin Jones (<a href="https://twitter.com/vcsjones">@vcsjones</a>) did a wonderful job of <a href="https://vcsjones.dev/2019/02/01/csharp-readonly-span-bytes-static/">digging into it</a>.<br />
We <strong>must</strong> use a <code class="highlighter-rouge">ReadOnlySpan&lt;byte&gt;</code> for the optimization to trigger: Not reading <em>that</em> fine-print cost me two nights of my life chasing what I was <em>sure</em> had to be a GC/JIT bug. Normally, it would be a <strong>bad</strong> idea to store a <code class="highlighter-rouge">ReadOnlySpan&lt;int&gt;</code> as a <code class="highlighter-rouge">ReadOnlySpan&lt;byte&gt;</code>: we are forced to choose between little/big-endian encoding <em>at compile-time</em>. This runs up against the fact that in C# we compile once and debug (and occasionally run :) everywhere. Therefore, we have to <em>assume</em> our binaries might run on both little/big-endian machines where the CPU might not match the encoding we chose.<br />
<strong>In this case</strong>, praise the vector deities, blessed be their name and all that they touch, this is a <em>non-issue</em>: The entire premise is <strong>x86</strong> specific. This means that this code will <strong>never</strong> run on a big-endian machine. We can simply assume little endianness here till the end of all times.</p>
        </div>
</td>
</tr>
</table>

</div>

<p>We’ve covered the basic layout of the permutation table. We’ll go back to it once we start optimization efforts in earnest on the 4<sup>th</sup> post, but for now, we can move on to the loop surrounding our vectorized partition block.</p>

<h2 id="double-pumped-loop">Double Pumped Loop</h2>

<p>Armed with a vectorized partitioning block, it’s time to hammer our unsorted array with it, but there’s a wrinkle: In-place sorting. This brings a new challenge to the table: If you followed the previous section carefully, you might have noticed it already. For every <code class="highlighter-rouge">Vector256&lt;int&gt;</code> we read, we ended up writing that same vector twice to both ends of the array. You don’t have to be a math wizard to figure out that if we end up writing 16 elements for every 8 we read, that doesn’t sound very in-placy, to begin with. Moreover, this extra writing would have to overwrite data that we have <em>not read yet</em>.<br />
Initially, it would seem, we’ve managed to position ourselves between a rock and a hard place.</p>

<p>But all is not lost! In reality, we immediately adjust the next write positions on both sides in such a way that their <strong>sum</strong> advances by 8. In other words, we are at risk of overwriting unread data only temporarily while we store the data back. I ended up adopting a tricky approach: We will need to continuously make sure we have at least 8 elements (the size of our block) of free space on <em>both</em> sides of the array so we could, in turn, perform a full, efficient 8-element write to both ends without overwriting a single bit of data we haven’t read yet.</p>

<p>Here’s a visual representation of the mental model I was in while debugging/making this work (I’ll note I had the same facial expressions as this poor Charmander while writing and debugging that code):</p>

<video controls="" playsinline="" loop="" preload="auto" width="100%">
    <source src="../talks/intrinsics-sorting-2019/fire.webm" type="video/webm" />
    <source src="../talks/intrinsics-sorting-2019/fire.mp4" type="video/mp4" />
    <img src="../talks/intrinsics-sorting-2019/fire.gif " alt="" />
</video>

<p><br /></p>

<p>Funny, right? It’s closer to what I actually do than I’d like to admit! I fondly named this approach in my code as “double-pumped partitioning”. It pumps values in-to/out-of <strong>both</strong> ends of the array at all times. I’ve left it pretty much intact in the repo under the name <a href="https://github.com/damageboy/VxSort/blob/research/VxSortResearch/Unstable/AVX2/Happy/00_DoublePumpNaive.cs"><code class="highlighter-rouge">DoublePumpNaive</code></a>, in case you want to dig through the full code. Like all good things in life, it comes in 3-parts:</p>

<ul>
  <li>Prime the pump (make some initial room inside the array).</li>
  <li>Loop over the data in 8-element chunks executing our vectorized code block.</li>
  <li>Finally, go over the last remaining data elements (e.g. the last remaining <code class="highlighter-rouge">&lt; 8</code> block of unpartitioned data) and partition them using scalar code. This is a very common and unfortunate pattern we find in vectorized code, as we need to finish off with just a bit of scalar work.</li>
</ul>

<p>Let’s start with another visual aid I ended up doing to better explain this; note the different color codes and legend I’ve provided here, and try to watch a few loops noticing the various color transitions, this will become useful as you parse the text and code below:</p>

<div>
  <div class="stickemup">
    <object class="animated-border" type="image/svg+xml" data="../talks/intrinsics-sorting-2019/double-pumped-loop-with-hint.svg"></object>
  </div>
  <object style="margin-top: 2em" type="image/svg+xml" data="../talks/intrinsics-sorting-2019/double-pumped-loop-legend.svg"></object>

  <ul>
    <li>Each rectangle is 8-elements wide.
      <ul>
        <li>Except for the middle one, which represents the last group of up to 8 elements that need to be partitioned. This is often called in vectorized parlance the “remainder problem”.</li>
      </ul>
    </li>
    <li>We want to partition the entire array, in-place, or turn it from <span style="padding: 1px; border: 1px solid black; border-radius: 2px; background-color: #db9d00ff">orange</span> into the green/red colors:
      <ul>
        <li><span style="padding: 1px; border: 1px solid black; border-radius: 2px; background-color: #bbe33d">Green</span>: for smaller-than-or-equal to the pivot values, on the left side.</li>
        <li><span style="padding: 1px; border: 1px solid black; border-radius: 2px; background-color: #c9211e; color: white">Red</span>: for greater-than-or-equal the pivot values, on the right side.</li>
      </ul>
    </li>
    <li>Initially we “prime the pump”, or make some room inside the array, by partitioning into some temporary memory, marked as the 3x8-element blocks in <span style="padding: 1px; border: 1px solid black; border-radius: 2px; background-color: #f67eec">purple</span>:
      <ul>
        <li>We allocate this temporary space somewhere on the stack; We’ll discuss why this isn’t really a big deal below.</li>
        <li>We read one vector’s worth of elements from the left and execute our partitioning block into the temporary space.</li>
        <li>We repeat the process for the right side.</li>
        <li>At this stage, one vector on each edge has already been partitioned, and their color is now <span style="padding: 1px; border: 1px solid black; border-radius: 2px; background-color:#b2b2b2ff">gray</span>, which represents data/area within our array we can freely <em>write</em> into.</li>
      </ul>
    </li>
    <li>From here-on, we’re in the main loop: this could go on for millions of iterations, even though in this animation we only see 4 iterations in total:
      <ul>
        <li>In every round, we <em>choose</em> where we read from next: From the left <em>-or-</em> right side of the <span style="padding: 1px; border: 1px solid black; border-radius: 2px; background-color: #db9d00ff">orange</span> area?<br />
How? Easy-peasy: Whichever side has a <strong>smaller</strong> <span style="padding: 1px; border: 1px solid black; border-radius: 2px; background-color:#b2b2b2ff">gray</span> area!
          <ul>
            <li><em>Intuition</em>: The gray area represents the distance between the head (read) and tail (write) pointers we set up for each side, the smaller the distance/area is, the more likely that our next 8-element partition <em>might</em> end with us overwriting that side’s head with the tail.</li>
            <li><strong>We really don’t want that to happen…</strong></li>
            <li>We read from the only side <em>where this might happen next</em>, thereby adding 8 more elements of breathing space to that side just in time before we cause a meltdown. (you can see this clearly in the animation as each orange block turns gray <em>after</em> we read it, <em>but before</em> we write to both sides…)</li>
          </ul>
        </li>
        <li>We partition the data inside the <code class="highlighter-rouge">Vector256&lt;int&gt;</code> we just read and write it to the next write position on each side.</li>
        <li>We advance each write pointer according to how much of that register was red/green, we’ve discussed the how of it when we toured the vectorized block. Here you can see the end result reflected in how the red portion of the written copy on the left-hand side turns into gray, and the green portion on the right-hand side turns into gray correspondingly.<br />
<strong>Remember</strong>: We’ve seen the code in detail when we previously discussed the partitioning block; I repeat it here since it is critical for understanding how the whole process clicks together.</li>
      </ul>
    </li>
    <li>For the finishing touch:
      <ul>
        <li>Left with less than 8 elements, we partition with plain old scalar code the few remaining elements, into the temporary memory area again.</li>
        <li>We copy back each side of the temporary area back to the main array, and we’re done!</li>
        <li>We move the pivot value that was left untouched all this time on the right edge of our segment and move it to where the new boundary is.</li>
      </ul>
    </li>
  </ul>

  <p>Let’s go over it again, in more detail, this time with code:</p>
</div>

<h3 id="setup-make-some-room">Setup: Make some room!</h3>

<p>What I eventually opted for was to read from <em>one</em> area and write to <em>another</em> area in the same array. But we need to make some spare room inside the array for this. How?</p>

<p>We cheat! (¯\<em>(ツ)</em>/¯), but not really: we allocate some temporary space on stack, by using the relatively new <code class="highlighter-rouge">ref struct</code> feature in C# in combination with <code class="highlighter-rouge">fixed</code> arrays, here’s why this isn’t really cheating in any reasonable person’s book:</p>

<ul>
  <li>Stack allocation doesn’t put pressure on the GC, and its allocation is super fast/slim.</li>
  <li>We allocate <em>once</em> at the top of our entire sort operation and reuse that space while recursing.</li>
  <li>“Just a bit” is really just a bit: For our 8-element partition block we need room for 1 x 8-elements vector on <strong>each</strong> side of the array, so we allocate a total of 2 x 8 integers. In addition, we allocate 8 more elements for handling the remainder (well technically, 7 would be enough, but I’m not a monster, I like round numbers just like the next person), so a total of 96 bytes. Not too horrid.</li>
</ul>

<p>Here’s the signature + setup code:</p>

<div>
  <div class="stickemup">

    <div class="language-csharp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td><td class="rouge-code"><pre><span class="k">unsafe</span> <span class="kt">int</span><span class="p">*</span> <span class="nf">VectorizedPartitionInPlace</span><span class="p">(</span><span class="kt">int</span><span class="p">*</span> <span class="n">left</span><span class="p">,</span> <span class="kt">int</span><span class="p">*</span> <span class="n">right</span><span class="p">)</span>
<span class="p">{</span>
    <span class="kt">var</span> <span class="n">N</span> <span class="p">=</span> <span class="n">Vector256</span><span class="p">&lt;</span><span class="n">T</span><span class="p">&gt;.</span><span class="n">Count</span><span class="p">;</span> <span class="c1">// Treated by JIT as constant!</span>

    <span class="kt">var</span> <span class="n">writeLeft</span> <span class="p">=</span> <span class="n">left</span><span class="p">;</span>
    <span class="kt">var</span> <span class="n">writeRight</span> <span class="p">=</span> <span class="n">right</span> <span class="p">-</span> <span class="n">N</span> <span class="p">-</span> <span class="m">1</span><span class="p">;</span>
    <span class="kt">var</span> <span class="n">tmpLeft</span> <span class="p">=</span> <span class="n">_tempStart</span><span class="p">;</span>
    <span class="kt">var</span> <span class="n">tmpRight</span> <span class="p">=</span> <span class="n">_tempEnd</span> <span class="p">-</span> <span class="n">N</span><span class="p">;</span>

    <span class="kt">var</span> <span class="n">pivot</span> <span class="p">=</span> <span class="p">*</span><span class="n">right</span><span class="p">;</span>
    <span class="kt">var</span> <span class="n">P</span> <span class="p">=</span> <span class="n">Vector256</span><span class="p">.</span><span class="nf">Create</span><span class="p">(</span><span class="n">pivot</span><span class="p">);</span>

    <span class="nf">PartitionBlock</span><span class="p">(</span><span class="n">left</span><span class="p">,</span>          <span class="n">P</span><span class="p">,</span> <span class="k">ref</span> <span class="n">tmpLeft</span><span class="p">,</span> <span class="k">ref</span> <span class="n">tmpRight</span><span class="p">);</span>
    <span class="nf">PartitionBlock</span><span class="p">(</span><span class="n">right</span> <span class="p">-</span> <span class="n">N</span> <span class="p">-</span> <span class="m">1</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="k">ref</span> <span class="n">tmpLeft</span><span class="p">,</span> <span class="k">ref</span> <span class="n">tmpRight</span><span class="p">);</span>

    <span class="kt">var</span> <span class="n">readLeft</span>  <span class="p">=</span> <span class="n">left</span> <span class="p">+</span> <span class="n">N</span><span class="p">;</span>
    <span class="kt">var</span> <span class="n">readRight</span> <span class="p">=</span> <span class="n">right</span> <span class="p">-</span> <span class="m">2</span><span class="p">*</span><span class="n">N</span> <span class="p">-</span> <span class="m">1</span><span class="p">;</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>

  </div>

  <p>The function accepts two parameters: <code class="highlighter-rouge">left</code>, <code class="highlighter-rouge">right</code> pointing to the edges of the partitioning task we were handed. The selected pivot is “passed” in an unconventional way: the caller (The top-level sort function) is responsible for <strong>moving</strong> it to the right edge of the array before calling the partitioning function. In other words, we start executing the function expecting the pivot to be already selected and placed at the right edge of the segment (e.g., <code class="highlighter-rouge">right</code> points to it). This is a remnant of my initial copy-pasting of CoreCLR code, and to be honest, I don’t care enough to change it.</p>

  <p>We start by setting up various pointers we’ll be using on <span class="uk-label">L5-8</span>: The <code class="highlighter-rouge">writeLeft</code> and <code class="highlighter-rouge">writeRight</code> pointers pointing into the internal edges of our array (excluding the last element which is pointing to the selected pivot), while the <code class="highlighter-rouge">tmpLeft</code> and <code class="highlighter-rouge">tmpRight</code> pointers are pointing into the internal edges of the temporary space.<br />
One recurring pattern is that the right-side pointers are pointing on vector’s worth on elements <strong>left</strong> of their respective edge. This makes sense given that we will be using vectorized write operations that take a pointer to memory and write 8 elements at a time; the pointers are setup accounting for that assymetry.</p>

  <table style="margin-bottom: 0em" class="notice--info">
<tr>
<td style="border: none; padding-top: 0; padding-bottom: 0; vertical-align: top"><span class="uk-label">Note</span></td>
<td style="border: none; padding-top: 0; padding-bottom: 0"><div>
          <p>I’m using a “variable” (<code class="highlighter-rouge">N</code>) on <span class="uk-label">L3</span> instead of <code class="highlighter-rouge">Vector256&lt;int&gt;.Count</code>. There’s a reason for those double quotes: At JIT time, the right-hand expression is considered as a constant as far as the JIT is concerned. Furthermore, once we initialize N with its value and <em>never</em> modify it, the JIT treats N as a constant as well! So really, I get to use a short/readable name and pay no penalty in for it.</p>
        </div>
</td>
</tr>
</table>

  <p>We proceed to partition a single 8-element vector on <em>each</em> side on <span class="uk-label">L13-14</span>, with our good-ole’ partitioning block <strong>straight into</strong> that temporary space through the pointers we just setup. It is important to remember that having done that, we don’t care about the original contents of the area we just read from anymore: we’re free to write up to one <code class="highlighter-rouge">Vector256&lt;T&gt;</code> to each edge of the array in the future. We’ve made enough room inside our array available for writing in-place while partitioning.</p>

  <p>We finish the setup on <span class="uk-label">L16-17</span> by initializing read pointers for every side (<code class="highlighter-rouge">readLeft</code>, <code class="highlighter-rouge">readRight</code>); An alternative way to think about these pointers is that each side gets its own head (read) and tail (write) pointers. We will be continuously reading from <strong>one</strong> of the heads and writing to <strong>both</strong> tails from now on.</p>

  <p>The setup ends with <code class="highlighter-rouge">readLeft</code> pointing a single <code class="highlighter-rouge">Vector256&lt;int&gt;</code> <em>right</em> of <code class="highlighter-rouge">left</code> , and <code class="highlighter-rouge">readRight</code> pointing 1 element + 2x<code class="highlighter-rouge">Vector256&lt;int&gt;</code> <em>left</em> of <code class="highlighter-rouge">right</code>. The setup of <code class="highlighter-rouge">readRight</code> might initially seem peculiar, but easily explained:</p>

  <ul>
    <li><code class="highlighter-rouge">right</code> itself points to the selected pivot; we’re not going to (re-)partition it, so we skip that element (this explains the <code class="highlighter-rouge">- 1</code>).</li>
    <li>As with the <code class="highlighter-rouge">tmpRight</code> and <code class="highlighter-rouge">writeWrite</code> pointers, when we read/write using <code class="highlighter-rouge">Avx2.LoadDquVector256</code>/<code class="highlighter-rouge">Avx.Store</code> we always have to supply the <em>start</em> address to read from or write to!<br />
Since There is no ability to read/write to the “left” of the pointer, we pre-decrement that pointer by <code class="highlighter-rouge">2*N</code> to account for the data that was already partitioned and to prepare it for the next read.</li>
  </ul>

  <h3 id="loop">Loop</h3>

  <p>Here’s the same loop we saw in the animation with our vectorized block smack in its middle, in plain-old C#:</p>
</div>
<div>
  <div class="stickemup">

    <div class="language-csharp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="rouge-code"><pre>    <span class="k">while</span> <span class="p">(</span><span class="n">readRight</span> <span class="p">&gt;=</span> <span class="n">readLeft</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="p">*</span><span class="n">nextPtr</span><span class="p">;</span>
        <span class="k">if</span> <span class="p">((</span><span class="n">readLeft</span>   <span class="p">-</span> <span class="n">writeLeft</span><span class="p">)</span> <span class="p">&lt;=</span> <span class="p">(</span><span class="n">writeRight</span> <span class="p">-</span> <span class="n">readRight</span><span class="p">))</span> <span class="p">{</span>
            <span class="n">nextPtr</span> <span class="p">=</span> <span class="n">readLeft</span><span class="p">;</span>
            <span class="n">readLeft</span> <span class="p">+=</span> <span class="n">N</span><span class="p">;</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="n">nextPtr</span> <span class="p">=</span> <span class="n">readRight</span><span class="p">;</span>
            <span class="n">readRight</span> <span class="p">-=</span> <span class="n">N</span><span class="p">;</span>
        <span class="p">}</span>

        <span class="nf">PartitionBlock</span><span class="p">(</span><span class="n">nextPtr</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="k">ref</span> <span class="n">writeLeft</span><span class="p">,</span> <span class="k">ref</span> <span class="n">writeRight</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="n">readRight</span> <span class="p">+=</span> <span class="n">N</span><span class="p">;</span>
    <span class="n">tmpRight</span> <span class="p">+=</span> <span class="n">N</span><span class="p">;</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>

  </div>

  <p>This is the heart of the partitioning operation and where we spend most of the time sorting the array. Looks quite boring, eh?</p>

  <p>This loop is all about calling our good ole’ partitioning block on the entire array. We-reuse the same block on <span class="uk-label">L11</span>, but here, for the first time, actually use it as an in-place partitioning block, since we are both reading and writing to the same array.<br />
While the runtime of the loop is dominated by the partitioning block, the interesting bit is that beefy condition on <span class="uk-label">L3</span> that we described/animated before: it calculates the distance between each head and tail on both sides and compares them to determine which side has less space left, or which side is closer to being overwritten. Given that the <strong>next</strong> read will happen from the side we choose here, we’ve just added 8 more integers worth of <em>writing</em> space to that same endangered side, thereby eliminating the risk of overwriting.<br />
While it might be easy to read in terms of correctness or motivation, this is a very <em>sad line of code</em>, as it will haunt us in the next posts!</p>

  <p>Finally, as we exit the loop once there are <code class="highlighter-rouge">&lt; 8</code> elements left (remember that we pre-decremented <code class="highlighter-rouge">readRight</code> by <code class="highlighter-rouge">N</code> elements before the loop), we are done with all vectorized work for this partitioning call. as such, this is as good a time to re-adjust both <code class="highlighter-rouge">readRight</code> and <code class="highlighter-rouge">tmpRight</code> that were pre-decremented by <code class="highlighter-rouge">N</code> elements to make them ready-to-go for the final step of handling the remainder with scalr sorting, on <span class="uk-label">L13-14</span>.</p>

  <h3 id="handling-the-remainder-and-finishing-up">Handling the remainder and finishing up</h3>

  <p>Here’s the final piece of this function:</p>
</div>
<div>
  <div class="stickemup">

    <div class="language-csharp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td><td class="rouge-code"><pre>    <span class="k">while</span> <span class="p">(</span><span class="n">readLeft</span> <span class="p">&lt;</span> <span class="n">readRight</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">var</span> <span class="n">v</span> <span class="p">=</span> <span class="p">*</span><span class="n">readLeft</span><span class="p">++;</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">v</span> <span class="p">&lt;=</span> <span class="n">pivot</span><span class="p">)</span> <span class="p">{</span>
            <span class="p">*</span><span class="n">tmpLeft</span><span class="p">++</span> <span class="p">=</span> <span class="n">v</span><span class="p">;</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="p">*--</span><span class="n">tmpRight</span> <span class="p">=</span> <span class="n">v</span><span class="p">;</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="kt">var</span> <span class="n">leftTmpSize</span> <span class="p">=</span> <span class="p">(</span><span class="kt">uint</span><span class="p">)</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="p">(</span><span class="n">tmpLeft</span> <span class="p">-</span> <span class="n">_tempStart</span><span class="p">);</span>
    <span class="n">Unsafe</span><span class="p">.</span><span class="nf">CopyBlockUnaligned</span><span class="p">(</span><span class="n">writeLeft</span><span class="p">,</span> <span class="n">_tmpStart</span><span class="p">,</span> <span class="n">leftTmpSize</span> <span class="p">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>
    <span class="n">writeLeft</span> <span class="p">+=</span> <span class="n">leftTmpSize</span><span class="p">;</span>
    <span class="kt">var</span> <span class="n">rightTmpSize</span> <span class="p">=</span> <span class="p">(</span><span class="kt">uint</span><span class="p">)</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="p">(</span><span class="n">_tempEnd</span> <span class="p">-</span> <span class="n">tmpRight</span><span class="p">);</span>
    <span class="n">Unsafe</span><span class="p">.</span><span class="nf">CopyBlockUnaligned</span><span class="p">(</span><span class="n">writeLeft</span><span class="p">,</span> <span class="n">tmpRight</span><span class="p">,</span> <span class="n">rightTmpSize</span> <span class="p">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>
    <span class="nf">Swap</span><span class="p">(</span><span class="n">writeLeft</span><span class="p">,</span> <span class="n">right</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">writeLeft</span><span class="p">;</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>

  </div>

  <p>Finally, we come out of the loop once we have less than 8-elements to partition (1-7 elements). We can’t use vectorized code here, so we drop to plain-old scalar partitioning on <span class="uk-label">L1-8</span>. To keep things simple, we partition these last elements straight into the temporary area. This is the reason we’re allocating 8 more elements in the temporary area in the first place.</p>

  <p>Once we’re done with this remainder nuisance, we copy back the already partitioned data from the temporary area back into the array to the area left between <code class="highlighter-rouge">writeLeft</code> and <code class="highlighter-rouge">writeRight</code>, it’s a quick 64-96 byte copy in two operations, performed <span class="uk-label">L10-14</span> and we are nearly done. We still need to move the pivot <em>back</em> to the newly calculated pivot position (remember the caller placed it on the right edge of the array as part of pivot selection) and report this position back as the return value for this to be officially be christened as AVX2 partitioning function.</p>
</div>

<h2 id="pretending-were-arraysort">Pretending we’re Array.Sort</h2>

<p>Now that we have a proper partitioning function, it’s time to string it into a quick-sort like dispatching function: This will be the entry point to our sort routine:</p>

<div>
  <div class="stickemup">

    <div class="language-csharp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td><td class="rouge-code"><pre><span class="k">public</span> <span class="k">static</span> <span class="k">class</span> <span class="nc">DoublePumpNaive</span>
<span class="p">{</span>
    <span class="k">public</span> <span class="k">static</span> <span class="k">unsafe</span> <span class="k">void</span> <span class="n">Sort</span><span class="p">&lt;</span><span class="n">T</span><span class="p">&gt;(</span><span class="n">T</span><span class="p">[]</span> <span class="n">array</span><span class="p">)</span> <span class="k">where</span> <span class="n">T</span> <span class="p">:</span> <span class="n">unmanaged</span><span class="p">,</span> <span class="n">IComparable</span><span class="p">&lt;</span><span class="n">T</span><span class="p">&gt;</span>
    <span class="p">{</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">array</span> <span class="p">==</span> <span class="k">null</span><span class="p">)</span>
            <span class="k">throw</span> <span class="k">new</span> <span class="nf">ArgumentNullException</span><span class="p">(</span><span class="k">nameof</span><span class="p">(</span><span class="n">array</span><span class="p">));</span>

        <span class="k">fixed</span> <span class="p">(</span><span class="n">T</span><span class="p">*</span> <span class="n">p</span> <span class="p">=</span> <span class="p">&amp;</span><span class="n">array</span><span class="p">[</span><span class="m">0</span><span class="p">])</span> <span class="p">{</span>
            <span class="k">if</span> <span class="p">(</span><span class="k">typeof</span><span class="p">(</span><span class="n">T</span><span class="p">)</span> <span class="p">==</span> <span class="k">typeof</span><span class="p">(</span><span class="kt">int</span><span class="p">))</span> <span class="p">{</span>
                <span class="kt">var</span> <span class="n">pi</span> <span class="p">=</span> <span class="p">(</span><span class="kt">int</span><span class="p">*)</span> <span class="n">p</span><span class="p">;</span>
                <span class="kt">var</span> <span class="n">sorter</span> <span class="p">=</span> <span class="k">new</span> <span class="nf">VxSortInt32</span><span class="p">(</span><span class="n">startPtr</span><span class="p">:</span> <span class="n">pi</span><span class="p">,</span> <span class="n">endPtr</span><span class="p">:</span> <span class="n">pi</span> <span class="p">+</span> <span class="n">array</span><span class="p">.</span><span class="n">Length</span> <span class="p">-</span> <span class="m">1</span><span class="p">);</span>
                <span class="n">sorter</span><span class="p">.</span><span class="nf">Sort</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">pi</span> <span class="p">+</span> <span class="n">array</span><span class="p">.</span><span class="n">Length</span> <span class="p">-</span> <span class="m">1</span><span class="p">);</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="k">const</span> <span class="kt">int</span> <span class="n">SLACK_PER_SIDE_IN_VECTORS</span> <span class="p">=</span> <span class="m">1</span><span class="p">;</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>

  </div>

  <p>Most of this is pretty dull code:</p>

  <ul>
    <li>We start with a top-level static class <code class="highlighter-rouge">DoublePumpNaive</code> containing a single <code class="highlighter-rouge">Sort</code> entry point accepting a normal managed array.</li>
    <li>We special case, relying on generic type elision, for  <code class="highlighter-rouge">typeof(int)</code>, newing up a <code class="highlighter-rouge">VxSortInt32</code> struct and finally calling its internal <code class="highlighter-rouge">.Sort()</code> method to initiate the recursive sorting.
      <ul>
        <li>This is a good time as any to remind, again, that for the time being, I only implemented vectorized sorting when <code class="highlighter-rouge">T</code> is <code class="highlighter-rouge">int</code>. To fully replace <code class="highlighter-rouge">Array.Sort()</code> more tweaked versions of this code will have to be written to eventually support unsigned integers, both larger and smaller than 32 bits as well as floating-point types.</li>
      </ul>
    </li>
  </ul>

  <p>Continuing on to <code class="highlighter-rouge">VxSortInt32</code> itself:</p>

</div>

<div>
  <div class="stickemup">

    <div class="language-csharp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre></td><td class="rouge-code"><pre>
    <span class="k">internal</span> <span class="k">unsafe</span> <span class="k">ref</span> <span class="k">struct</span> <span class="nc">VxSortInt32</span>
    <span class="p">{</span>
        <span class="k">const</span> <span class="kt">int</span> <span class="n">SLACK_PER_SIDE_IN_ELEMENTS</span>    <span class="p">=</span> <span class="n">SLACK_PER_SIDE_IN_VECTORS</span> <span class="p">*</span> <span class="m">8</span><span class="p">;</span>
        <span class="k">const</span> <span class="kt">int</span> <span class="n">TMP_SIZE_IN_ELEMENTS</span>          <span class="p">=</span> <span class="m">2</span> <span class="p">*</span> <span class="n">SLACK_PER_SIDE_IN_ELEMENTS</span> <span class="p">+</span> <span class="m">8</span><span class="p">;</span>
        <span class="k">const</span> <span class="kt">int</span> <span class="n">SMALL_SORT_THRESHOLD_ELEMENTS</span> <span class="p">=</span> <span class="m">16</span><span class="p">;</span>

        <span class="k">readonly</span> <span class="kt">int</span><span class="p">*</span> <span class="n">_startPtr</span><span class="p">,</span>  <span class="n">_endPtr</span><span class="p">;</span>
                      <span class="n">_tempStart</span><span class="p">,</span> <span class="n">_tempEnd</span><span class="p">;</span>
        <span class="k">fixed</span> <span class="kt">int</span> <span class="n">_temp</span><span class="p">[</span><span class="n">TMP_SIZE_IN_ELEMENTS</span><span class="p">];</span>

        <span class="k">public</span> <span class="nf">VxSortInt32</span><span class="p">(</span><span class="kt">int</span><span class="p">*</span> <span class="n">startPtr</span><span class="p">,</span> <span class="kt">int</span><span class="p">*</span> <span class="n">endPtr</span><span class="p">)</span> <span class="p">:</span> <span class="k">this</span><span class="p">()</span>
        <span class="p">{</span>
            <span class="n">_startPtr</span> <span class="p">=</span> <span class="n">startPtr</span><span class="p">;</span>
            <span class="n">_endPtr</span>   <span class="p">=</span> <span class="n">endPtr</span><span class="p">;</span>
            <span class="k">fixed</span> <span class="p">(</span><span class="kt">int</span><span class="p">*</span> <span class="n">pTemp</span> <span class="p">=</span> <span class="n">_temp</span><span class="p">)</span> <span class="p">{</span>
                <span class="n">_tempStart</span> <span class="p">=</span> <span class="n">pTemp</span><span class="p">;</span>
                <span class="n">_tempEnd</span>   <span class="p">=</span> <span class="n">pTemp</span> <span class="p">+</span> <span class="n">TMP_SIZE_IN_ELEMENTS</span><span class="p">;</span>
            <span class="p">}</span>
        <span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>
  </div>

  <p>This is where the real top-level sorting entry point for 32-bit signed integers is:</p>

  <ul>
    <li>This struct contains a bunch of constants and members that are initialized for a single sort-job/call and immediately discarded once sorting is complete.</li>
    <li>There’s a little semingly nasty bit hiding in plain sight there, where we exfiltrate an interior pointer obtained inside a <code class="highlighter-rouge">fixed</code> block and store it for the lifetime of the struct, outside of the <code class="highlighter-rouge">fixed</code> block.
      <ul>
        <li>This is generally a no-no, since, in theory, we don’t have a guarantee that the struct won’t be boxed/stored inside a managed object on a heap where the GC is free to move our memory around.</li>
        <li>In this case, we <em>are ensuring</em> that instances of <code class="highlighter-rouge">VxSortInt32</code> are never promoted to the managed heap by declaring it as a <a href="https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/keywords/ref#ref-struct-types"><code class="highlighter-rouge">ref struct</code></a>.</li>
        <li>The motivation behind this is to ensure that the <code class="highlighter-rouge">fixed</code> temporary memory resides close to the other struct fields, taking advantage of <a href="https://en.wikipedia.org/wiki/Locality_of_reference">locality of reference</a>.</li>
      </ul>
    </li>
  </ul>

</div>

<div class="language-csharp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
</pre></td><td class="rouge-code"><pre>        <span class="k">internal</span> <span class="k">void</span> <span class="nf">Sort</span><span class="p">(</span><span class="kt">int</span><span class="p">*</span> <span class="n">left</span><span class="p">,</span> <span class="kt">int</span><span class="p">*</span> <span class="n">right</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="kt">var</span> <span class="n">length</span> <span class="p">=</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="p">(</span><span class="n">right</span> <span class="p">-</span> <span class="n">left</span> <span class="p">+</span> <span class="m">1</span><span class="p">);</span>

            <span class="kt">int</span><span class="p">*</span> <span class="n">mid</span><span class="p">;</span>
            <span class="k">switch</span> <span class="p">(</span><span class="n">length</span><span class="p">)</span> <span class="p">{</span>
                <span class="k">case</span> <span class="m">0</span><span class="p">:</span>
                <span class="k">case</span> <span class="m">1</span><span class="p">:</span>
                    <span class="k">return</span><span class="p">;</span>
                <span class="k">case</span> <span class="m">2</span><span class="p">:</span>
                    <span class="nf">SwapIfGreater</span><span class="p">(</span><span class="n">left</span><span class="p">,</span> <span class="n">right</span><span class="p">);</span>
                    <span class="k">return</span><span class="p">;</span>
                <span class="k">case</span> <span class="m">3</span><span class="p">:</span>
                    <span class="n">mid</span> <span class="p">=</span> <span class="n">right</span> <span class="p">-</span> <span class="m">1</span><span class="p">;</span>
                    <span class="nf">SwapIfGreater</span><span class="p">(</span><span class="n">left</span><span class="p">,</span> <span class="n">mid</span><span class="p">);</span>
                    <span class="nf">SwapIfGreater</span><span class="p">(</span><span class="n">left</span><span class="p">,</span> <span class="n">right</span><span class="p">);</span>
                    <span class="nf">SwapIfGreater</span><span class="p">(</span><span class="n">mid</span><span class="p">,</span>  <span class="n">right</span><span class="p">);</span>
                    <span class="k">return</span><span class="p">;</span>
            <span class="p">}</span>

            <span class="c1">// Go to insertion sort below this threshold</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">length</span> <span class="p">&lt;=</span> <span class="n">SMALL_SORT_THRESHOLD_ELEMENTS</span><span class="p">)</span> <span class="p">{</span>
                <span class="nf">InsertionSort</span><span class="p">(</span><span class="n">left</span><span class="p">,</span> <span class="n">right</span><span class="p">);</span>
                <span class="k">return</span><span class="p">;</span>
            <span class="p">}</span>

            <span class="c1">// Compute median-of-three, of:</span>
            <span class="c1">// the first, mid and one before last elements</span>
            <span class="n">mid</span> <span class="p">=</span> <span class="n">left</span> <span class="p">+</span> <span class="p">((</span><span class="n">right</span> <span class="p">-</span> <span class="n">left</span><span class="p">)</span> <span class="p">/</span> <span class="m">2</span><span class="p">);</span>
            <span class="nf">SwapIfGreater</span><span class="p">(</span><span class="n">left</span><span class="p">,</span> <span class="n">mid</span><span class="p">);</span>
            <span class="nf">SwapIfGreater</span><span class="p">(</span><span class="n">left</span><span class="p">,</span> <span class="n">right</span> <span class="p">-</span> <span class="m">1</span><span class="p">);</span>
            <span class="nf">SwapIfGreater</span><span class="p">(</span><span class="n">mid</span><span class="p">,</span>  <span class="n">right</span> <span class="p">-</span> <span class="m">1</span><span class="p">);</span>

            <span class="c1">// Pivot is mid, place it in the right hand side</span>
            <span class="nf">Swap</span><span class="p">(</span><span class="n">mid</span><span class="p">,</span> <span class="n">right</span><span class="p">);</span>

            <span class="kt">var</span> <span class="n">boundary</span> <span class="p">=</span> <span class="nf">VectorizedPartitionInPlace</span><span class="p">(</span><span class="n">left</span><span class="p">,</span> <span class="n">right</span><span class="p">);</span>

            <span class="nf">Sort</span><span class="p">(</span> <span class="n">left</span><span class="p">,</span> <span class="n">boundary</span> <span class="p">-</span> <span class="m">1</span><span class="p">);</span>
            <span class="nf">Sort</span><span class="p">(</span><span class="n">boundary</span> <span class="p">+</span> <span class="m">1</span><span class="p">,</span>  <span class="n">right</span><span class="p">);</span>
        <span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Lastly, we have the <code class="highlighter-rouge">Sort</code> method for the <code class="highlighter-rouge">VxSortInt32</code> struct. Most of this is code I blatantly copied for <a href="https://github.com/dotnet/coreclr/blob/master/src/System.Private.CoreLib/shared/System/Collections/Generic/ArraySortHelper.cs#L182"><code class="highlighter-rouge">ArraySortHelper&lt;T&gt;</code></a>. What it does is:</p>

<ul>
  <li>Special case for lengths of 0-3.</li>
  <li>When length <code class="highlighter-rouge">&lt;= 16</code> we just go straight to <code class="highlighter-rouge">InsertionSort</code> and skip all the recursive jazz (go back to post 1 if you want to know why <code class="highlighter-rouge">Array.Sort()</code> does this).</li>
  <li>When we have <code class="highlighter-rouge">&gt;= 17</code> elements, we go to vectorized partitioning:
    <ul>
      <li>We do median of 3 pivot selection.</li>
      <li>Swap that pivot so that it resides on the right-most index of the partition.</li>
    </ul>
  </li>
  <li>Call <code class="highlighter-rouge">VectorizedPartitionInPlace</code>, which we’ve seen before.
    <ul>
      <li>We conveniently take advantage of the fact we have <code class="highlighter-rouge">InsertionSort</code> to cover us for the small partitions, and our partitioning code can always assume that it can prime the pump with at least two vectors worth of vectorized partitioning without additional checks…</li>
    </ul>
  </li>
  <li>Recurse to the left.</li>
  <li>Recurse to the right.</li>
</ul>

<h2 id="initial-performance">Initial Performance</h2>

<p>Are we fast yet?</p>

<p>Yes! This is by no means the end, on the contrary, this is only a rather impressive beginning. We finally have something working, and it is even not entirely unpleasant, if I may say so:</p>

<div>
  <div class="stickemup">

<ul class="uk-tab" data-uk-switcher="{connect:'#295d691a-64b6-4806-8e0d-861458224cd0'}">

	<li class="uk-active"><a href="#"><i class="glyphicon glyphicon-stats"></i> Scaling</a></li>

	<li><a href="#"><i class="glyphicon glyphicon-stats"></i> Time/N</a></li>

	<li><a href="#"><i class="glyphicon glyphicon-list-alt"></i> Benchmarks</a></li>

	<li><a href="#"><i class="glyphicon glyphicon-list-alt"></i> Stats</a></li>

	<li><a href="#"><i class="glyphicon glyphicon-info-sign"></i> Setup</a></li>

</ul>

<ul id="295d691a-64b6-4806-8e0d-861458224cd0" class="uk-switcher uk-margin">

	<li>
<div>
<button class="helpbutton" data-toggle="chardinjs" onclick="$('body').chardinJs('start')"><object style="pointer-events: none;" type="image/svg+xml" data="/assets/images/help.svg"></object></button>
<div data-intro="Size of the sorting problem, 10..10,000,000 in powers of 10" data-position="bottom">
<div data-intro="Performance scale: Array.Sort (solid gray) is always 100%, and the other methods are scaled relative to it" data-position="left">
<div data-intro="Click legend items to show/hide series" data-position="right">
<div class="benchmark-chart-container">
<canvas data-chart="line">
N,100,1K,10K,100K,1M,10M
ArraySort,         1   , 1   , 1  , 1   , 1    , 1
DoublePumpedNaive, 1.67, 0.77, 0.6, 0.50, 0.39 , 0.36
<!-- 
{ 
 "data" : {
  "datasets" : [ { 
    "backgroundColor": "rgba(66,66,66,0.35)",
    "rough": { "fillStyle": "hachure", "hachureAngle": -30, "hachureGap": 9, "fillWeight": 0.3	}
  },
  { 
    "backgroundColor": "rgba(33,33,220,.9)",
    "rough": { "fillStyle": "hachure", "hachureAngle": 30, "hachureGap": 6	}
  }]
 },
 "options": {
    "title": { "text": "AVX2 Naive Sorting - Scaled to Array.Sort", "display": true },
    "scales": { 
      "yAxes": [{
       "ticks": {
         "fontFamily": "Indie Flower",
         "min": 0.2, 
         "callback": "ticksPercent"
        },
        "scaleLabel": {
          "labelString": "Scaling (%)",
          "display": true
        }
      }]
    }
 },
 "defaultOptions": {"scales":{"xAxes":[{"scaleLabel":{"display":"true,","labelString":"N (elements)","fontFamily":"Indie Flower"},"ticks":{"fontFamily":"Indie Flower"}}]},"legend":{"display":true,"position":"bottom","labels":{"fontFamily":"Indie Flower","fontSize":14}},"title":{"position":"top","fontFamily":"Indie Flower","fontSize":16}}
}
--> </canvas>
</div>
</div>
</div>
</div>
</div>

</li>

	<li>
<div>
<button class="helpbutton" data-toggle="chardinjs" onclick="$('body').chardinJs('start')"><object style="pointer-events: none;" type="image/svg+xml" data="/assets/images/help.svg"></object></button>
<div data-intro="Size of the sorting problem, 10..10,000,000 in powers of 10" data-position="bottom">
<div data-intro="Time in nanoseconds spent sorting per element. Array.Sort (solid gray) is the baseline, again" data-position="left">
<div data-intro="Click legend items to show/hide series" data-position="right">
<div class="benchmark-chart-container">
<canvas data-chart="line">
N,100,1K,10K,100K,1M,10M
ArraySort        , 19.9202, 35.4067, 52.3293, 64.6518, 70.5598, 81.0416
DoublePumpedNaive, 35.4138, 26.9828, 31.5477, 32.1774, 27.8901, 29.4917
<!-- 
{ 
 "data" : {
  "datasets" : [ { 
    "backgroundColor": "rgba(66,66,66,0.35)",
    "rough": { "fillStyle": "hachure", "hachureAngle": -30, "hachureGap": 9, "fillWeight": 0.3	}
  },
  { 
    "backgroundColor": "rgba(33,33,220,.9)",
    "rough": { "fillStyle": "hachure", "hachureAngle": 30, "hachureGap": 6	}
  }]
 },
 "options": {
    "title": { "text": "Array.Sort + AVX2 Naive Sorting - log(Time/N)", "display": true },
    "scales": { 
      "yAxes": [{ 
        "type": "logarithmic",
        "ticks": {
          "callback": "ticksNumStandaard",
          "fontFamily": "Indie Flower"          
        },
        "scaleLabel": {
          "labelString": "Time/N (ns)",
          "fontFamily": "Indie Flower",
          "display": true
        }
      }]
    }
 },
 "defaultOptions": {"scales":{"xAxes":[{"scaleLabel":{"display":"true,","labelString":"N (elements)","fontFamily":"Indie Flower"},"ticks":{"fontFamily":"Indie Flower"}}]},"legend":{"display":true,"position":"bottom","labels":{"fontFamily":"Indie Flower","fontSize":14}},"title":{"position":"top","fontFamily":"Indie Flower","fontSize":16}}
}
--> </canvas>
</div>
</div>
</div>
</div>
</div>
</li>

	<li>
<div>
<button class="helpbutton" data-toggle="chardinjs" onclick="$('body').chardinJs('start')"><object style="pointer-events: none;" type="image/svg+xml" data="/assets/images/help.svg"></object></button>
<table class="table datatable" data-json="../_posts/Bench.BlogPt3_Int32_-report.datatable.json" data-id-field="name" data-pagination="false" data-page-list="[9, 18]" data-intro="Each row in this table represents a benchmark result" data-position="left" data-show-pagination-switch="false">
  <thead data-intro="The header can be used to sort/filter by clicking" data-position="right">
    <tr>
        <th data-field="TargetMethodColumn.Method" data-sortable="true" data-filter-control="select">
          <span data-intro="The name of the benchmarked method" data-position="top">
            Method<br />Name
          </span>
        </th>
        <th data-field="N" data-sortable="true" data-value-type="int" data-filter-control="select">
            <span data-intro="The size of the sorting problem being benchmarked (# of integers)" data-position="top">
            Problem<br />Size
            </span>
        </th>
        <th data-field="TimePerNDataTable" data-sortable="true" data-value-type="float2-interval-muted">
            <span data-intro="Time in nanoseconds spent sorting each element in the array (with confidence intervals in parenthesis)" data-position="top">
              Time /<br />Element (ns)
            </span>
        </th>
        <th data-field="RatioDataTable" data-sortable="true" data-value-type="inline-bar-horizontal-percentage">
            <span data-intro="Each result is scaled to its baseline (Array.Sort in this case)" data-position="top">
                  Scaling
            </span>
        </th>
        <th data-field="Measurements" data-sortable="true" data-value-type="inline-bar-vertical">
            <span data-intro="Raw benchmark results visualize how stable the result it. Longest/Shortest runs marked with &lt;span style='color: red'&gt;Red&lt;/span&gt;/&lt;span style='color: green'&gt;Green&lt;/span&gt;" data-position="top">Measurements</span>
        </th>
    </tr>
  </thead>
</table>
</div>

</li>

	<li>
<div>
<button class="helpbutton" data-toggle="chardinjs" onclick="$('body').chardinJs('start')"><object style="pointer-events: none;" type="image/svg+xml" data="/assets/images/help.svg"></object></button>

<table class="table datatable" data-json="../_posts/unmanaged-vs-doublepumpednaive-stats.json" data-id-field="name" data-pagination="false" data-intro="Each row in this table contains statistics collected &amp; averaged out of thousands of runs with random data" data-position="left" data-show-pagination-switch="false">
  <thead data-intro="The header can be used to sort/filter by clicking" data-position="right">
    <tr>
        <th data-field="MethodName" data-sortable="true" data-filter-control="select">
          <span data-intro="The name of the benchmarked method" data-position="top">Method<br />Name</span>
        </th>
        <th data-field="ProblemSize" data-sortable="true" data-value-type="int" data-filter-control="select">
            <div data-intro="The size of the sorting problem being benchmarked (# of integers)" data-position="bottom" class="rotated-header-container">
            <div class="rotated-header">Size</div>
            </div>
        </th>
        <th data-field="MaxDepthScaledDataTable" data-sortable="true" data-value-type="inline-bar-horizontal">
            <div data-intro="The maximal depth of recursion reached while sorting" data-position="top" class="rotated-header-container">
              <div class="rotated-header">Max</div>
              <div class="rotated-header">Depth</div>
            </div>
        </th>
        <th data-field="NumPartitionOperationsScaledDataTable" data-sortable="true" data-value-type="inline-bar-horizontal">
            <div data-intro="# of partitioning operations per sort" data-position="bottom" class="rotated-header-container">
              <div class="rotated-header">Part</div>
              <div class="rotated-header">itions</div>
            </div>
        </th>
        <th data-field="NumVectorizedLoadsScaledDataTable" data-sortable="true" data-value-type="inline-bar-horizontal">
            <div data-intro="# of vectorized load operations" data-position="top" class="rotated-header-container">
              <div class="rotated-header">Vector</div>
              <div class="rotated-header">Loads</div>
            </div>
        </th>
        <th data-field="NumVectorizedStoresScaledDataTable" data-sortable="true" data-value-type="inline-bar-horizontal">
            <div data-intro="# of vectorized store operations" data-position="bottom" class="rotated-header-container">
              <div class="rotated-header">Vector</div>
              <div class="rotated-header">Stores</div>
            </div>
        </th>
        <th data-field="NumPermutationsScaledDataTable" data-sortable="true" data-value-type="inline-bar-horizontal">
            <div data-intro="# of vectorized permutation operations" data-position="top" class="rotated-header-container">
              <div class="rotated-header">Vector</div>
              <div class="rotated-header">Permutes</div>
            </div>
        </th>
        <th data-field="AverageSmallSortSizeScaledDataTable" data-sortable="true" data-value-type="inline-bar-horizontal">
            <div data-intro="For hybrid sorting, the average size that each small sort operation was called with (e.g. InsertionSort)" data-position="bottom" class="rotated-header-container">
              <div class="rotated-header">Small</div>
              <div class="rotated-header">Sort</div>
              <div class="rotated-header">Size</div>
            </div>
        </th>
        <th data-field="NumScalarComparesScaledDataTable" data-sortable="true" data-value-type="inline-bar-horizontal">
            <div data-intro="How many branches were executed in each sort operation that were based on the unsorted array elements" data-position="top" class="rotated-header-container">
              <div class="rotated-header">Data</div>
              <div class="rotated-header">Based</div>
              <div class="rotated-header">Branches</div>
            </div>
        </th>
        <th data-field="PercentSmallSortCompares" data-sortable="true" data-value-type="float2-percentage">
            <div data-intro="What percent of&lt;/br&gt;⬅&lt;br/&gt;branches happenned as part of small-sorts" data-position="bottom" class="rotated-header-container">
              <div class="rotated-header">Small</div>
              <div class="rotated-header">Sort</div>
              <div class="rotated-header">Branches</div>
            </div>
        </th>
    </tr>
  </thead>
</table>
</div>
</li>

	<li>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="rouge-code"><pre><span class="nv">BenchmarkDotNet</span><span class="o">=</span>v0.12.0, <span class="nv">OS</span><span class="o">=</span>clear-linux-os 32120
Intel Core i7-7700HQ CPU 2.80GHz <span class="o">(</span>Kaby Lake<span class="o">)</span>, 1 CPU, 4 logical and 4 physical cores
.NET Core <span class="nv">SDK</span><span class="o">=</span>3.1.100
  <span class="o">[</span>Host]     : .NET Core 3.1.0 <span class="o">(</span>CoreCLR 4.700.19.56402, CoreFX 4.700.19.56404<span class="o">)</span>, X64 RyuJIT
  Job-DEARTS : .NET Core 3.1.0 <span class="o">(</span>CoreCLR 4.700.19.56402, CoreFX 4.700.19.56404<span class="o">)</span>, X64 RyuJIT

<span class="nv">InvocationCount</span><span class="o">=</span>3  <span class="nv">IterationCount</span><span class="o">=</span>15  <span class="nv">LaunchCount</span><span class="o">=</span>2
<span class="nv">UnrollFactor</span><span class="o">=</span>1  <span class="nv">WarmupCount</span><span class="o">=</span>10

<span class="nv">$ </span><span class="nb">grep</span> <span class="s1">'stepping\|model\|microcode'</span> /proc/cpuinfo | <span class="nb">head</span> <span class="nt">-4</span>
model           : 158
model name      : Intel<span class="o">(</span>R<span class="o">)</span> Core<span class="o">(</span>TM<span class="o">)</span> i7-7700HQ CPU @ 2.80GHz
stepping        : 9
microcode       : 0xb4
</pre></td></tr></tbody></table></code></pre></div></div>

</li>

</ul>

</div>

  <p>We’re off to a very good start:</p>

  <ul>
    <li>
      <p>We can see that as soon as we hit 1000 element arrays (even earlier, in earnest), we already outperform <code class="highlighter-rouge">Array.Sort</code> (87% runtime), and by the time we get to 1M / 10M element arrays, we see speed-ups north of 2.5x (39%, 37% runtime) over the scalar C++ code!</p>
    </li>
    <li>
      <p>While <code class="highlighter-rouge">Array.Sort</code> is behaving like we would expect from a <code class="highlighter-rouge">QuickSort</code>-like function: it is slowing down at rate you’d expect given that it has a Big-O notation of \(\mathcal{O}(n\log{}n)\), our own <code class="highlighter-rouge">DoublePumpedNaive</code> is peculiar: The time spent sorting every single element starts going up as we increase <code class="highlighter-rouge">N</code>, then goes down a bit and back up. Huh? It actually improves as we sort more data? Quite unreasonable, unless we remind ourselves that we are executing a mix of scalar insertion sort and vectorized code. Where are we actually spending more CPU cycles though?  We’ll run some profiling sessions in a minute, to get a better idea of what’s going on.</p>
    </li>
  </ul>

  <p>If you recall, on the first post in this series, I presented some statistics about is going on inside our sort routine. This is a perfect time to switch to the statistics tab, where I’ve beefed up the table with some vectorized counters that didn’t make sense before with the scalar version. From here we can learn a few interesting facts:</p>

  <ul>
    <li>The number of partitioning operations / small sorts is practically the same
      <ul>
        <li>You could ask yourself, or me, why they are not <strong>exactly</strong> the same?
To which I’d answer:
          <ul>
            <li>The thresholds are 16 vs. 17, which has some effect.</li>
            <li>We have to remember that the resulting partitions from each implementation end up looking slightly different because of the double pumping + temporary memory shenanigans. Once the partitions look different, the following pivots selected are different, and the entire whole sort mechanic looks slightly different.</li>
          </ul>
        </li>
      </ul>
    </li>
    <li>We are doing a lot of vectorized work:
      <ul>
        <li>Loading two vectors per 8-element(1 data vector + 1 permutation vector)</li>
        <li>Storing two vectors (left+right) for every vector read</li>
        <li>In a weird coincidence, this means we perform the same number of vectorized loads and stores for every test case.<br />
In future posts, I will discard one of these columns to reduce the amount of information load…</li>
        <li>Finally, lest we forget, we perfom compares/permutations at exactly half of the load/store rate.</li>
      </ul>
    </li>
    <li>All of this is helping us by reducing the number of scalar comparisons, but there’s still quite a lot of it left too:
      <ul>
        <li>We continue to do scalar partitioning inside <code class="highlighter-rouge">VectorizedPartitionInPlace</code>, as part of handling the remainder that doesn’t fit into a <code class="highlighter-rouge">Vector256&lt;int&gt;</code>.</li>
        <li>We are still executing scalar comparisons as part of small-sorting/inside of the insertion sort at an alarming rate:
          <ul>
            <li>The absolute number of comparisons is quite high: We’re still doing millions of data-based branches.</li>
            <li>It is also clear from the counters that the overwhelming majority of these are from <code class="highlighter-rouge">InsertionSort</code>: If we focus on the 1M/10M cases here,    we see that <code class="highlighter-rouge">InsertionSort</code> went up from attributing 28.08%/24.60% of scalar comparisons in the <code class="highlighter-rouge">Unmanaged</code> (scalar) test-case all the way to 66.4%/62.74% in the vectorized <code class="highlighter-rouge">DoublePumpNaive</code> version. Of course this rise is merely in percent terms, but clearly we will have to deal with this if we intend to make this thing fast(er).</li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>

  <p>This is but the beginning of our profiling journey, but we are already learning a complicated truth: Right now, as fast as this is already going, the scalar code we use for insertion sort will always put an upper limit on how fast we can possibly go by optimizing the <em>vectorized code</em> we’ve gone over so far, <em>unless</em> we get rid of <code class="highlighter-rouge">InsertionSort</code> alltogether, replacing it with something better. But first thing’s first, we must remain focused: 65% of instructions executed are still spent doing vectorized partitioning; That is the biggest target on our scope!</p>
</div>

<p>As promised, it’s time we profile the code to see what’s really up: We can fire up the venerable Linux <code class="highlighter-rouge">perf</code> tool, through a simple test binary/project I’ve coded up which allows me to execute some dummy sorting by selecting the sort method I want to invoke and specify some parameters for it through the command line, for example:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="nv">$ </span><span class="nb">cd</span> ~/projects/public/VxSort/Example
<span class="nv">$ </span>dotnet publish <span class="nt">-c</span> release <span class="nt">-o</span> linux-x64 <span class="nt">-r</span> linux-x64
<span class="c"># Run AVX2DoublePumped with 1,000,000 elements x 100 times</span>
<span class="nv">$ </span>./linux-x64/Example <span class="nt">--type-list</span> DoublePumpNaive <span class="nt">--size-list</span> 1000000
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Here we call the <code class="highlighter-rouge">DoublePumpedNaive</code> implementation we’ve been discussing from the beginning of this post with 1M elements, and sort the random data 100 times to generate some heat in case global warming is not cutting it for you.<br />
I know that calling <code class="highlighter-rouge">dotnet publish ...</code> seems superfluous, but trust<sup id="fnref:0" role="doc-noteref"><a href="#fn:0" class="footnote">1</a></sup> me and go with me on this one:</p>

<ul class="uk-tab" data-uk-switcher="{connect:'#26856539-6dda-4083-b43d-3ef0d6fd4232'}">

	<li class="uk-active"><a href="#">1M</a></li>

	<li><a href="#">10K</a></li>

</ul>

<ul id="26856539-6dda-4083-b43d-3ef0d6fd4232" class="uk-switcher uk-margin">

	<li>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="rouge-code"><pre><span class="nv">$ COMPlus_PerfMapEnabled</span><span class="o">=</span>1  perf record <span class="nt">-F</span> max <span class="nt">-e</span> instructions ./Example <span class="se">\</span>
       <span class="nt">--type-list</span> DoublePumpedNaive <span class="nt">--size-list</span> 1000000
...
<span class="nv">$ </span>perf report <span class="nt">--stdio</span> <span class="nt">-F</span> overhead,sym | <span class="nb">head</span> <span class="nt">-15</span>
...
<span class="c"># Overhead  Symbol</span>
    65.66%  <span class="o">[</span>.] ... ::VectorizedPartitionInPlace<span class="o">(</span>int32<span class="k">*</span>,int32<span class="k">*</span>,int32<span class="k">*</span><span class="o">)[</span>Optimized]
    22.43%  <span class="o">[</span>.] ... ::InsertionSort<span class="o">(!!</span>0<span class="k">*</span>,!!0<span class="k">*</span><span class="o">)[</span>Optimized]
     5.43%  <span class="o">[</span>.] ... ::QuickSortInt<span class="o">(</span>int32<span class="k">*</span>,int32<span class="k">*</span>,int32<span class="k">*</span>,int32<span class="k">*</span><span class="o">)[</span>OptimizedTier1]
     4.00%  <span class="o">[</span>.] ... ::Memmove<span class="o">(</span>uint8&amp;,uint8&amp;,uint64<span class="o">)[</span>OptimizedTier1]
</pre></td></tr></tbody></table></code></pre></div></div>

</li>

	<li>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="rouge-code"><pre><span class="nv">$ COMPlus_PerfMapEnabled</span><span class="o">=</span>1 perf record <span class="nt">-F</span> max <span class="nt">-e</span> instructions ./Example <span class="se">\</span>
       <span class="nt">--type-list</span> AVX2DoublePumpedNaive <span class="nt">--size-list</span> 10000
...
<span class="nv">$ </span>perf report <span class="nt">--stdio</span> <span class="nt">-F</span> overhead,sym | <span class="nb">head</span> <span class="nt">-15</span>
...
<span class="c"># Overhead  Symbol</span>
    54.59%  <span class="o">[</span>.] ... ::VectorizedPartitionInPlace<span class="o">(</span>int32<span class="k">*</span>,int32<span class="k">*</span>,int32<span class="k">*</span><span class="o">)[</span>Optimized]
    29.87%  <span class="o">[</span>.] ... ::InsertionSort<span class="o">(!!</span>0<span class="k">*</span>,!!0<span class="k">*</span><span class="o">)[</span>Optimized]
     7.02%  <span class="o">[</span>.] ... ::QuickSortInt<span class="o">(</span>int32<span class="k">*</span>,int32<span class="k">*</span>,int32<span class="k">*</span>,int32<span class="k">*</span><span class="o">)[</span>OptimizedTier1]
     5.23%  <span class="o">[</span>.] ... ::Memmove<span class="o">(</span>uint8&amp;,uint8&amp;,uint64<span class="o">)[</span>OptimizedTier1]
</pre></td></tr></tbody></table></code></pre></div></div>

</li>

</ul>

<p>This is a trimmed summary of <code class="highlighter-rouge">perf</code> session recording performance metrics, specifically: number of instructions executed for running a 1M element sort 100 times, followed by running a 10K element sort, 10K times. I was shocked when I saw this for the first time, but we’re starting to understand the previous oddities we saw with the <code class="highlighter-rouge">Time/N</code> column!<br />
We’re spending upwards of 20% of our time doing scalar insertion sorting! I lured you here with promises of vectorized sorting and yet, somehow, “only” 65% of the time is spent in doing “vectorized” work (which also has some scalar partitioning, if we’re honest). Not only that, but as the size of the array decreases, the percentage of time spent in scalar code <em>increases</em> (from 22.43% to 29.87%), which should not surprise us anymore.<br />
Before anything else, let me clearly state that this is not necessarily a bad thing! As the size of the partition decreases, the <em>benefit</em> of doing vectorized partitioning decreases in general, and even more so for our AVX2 partitioning, which has non-trivial start-up overhead. We shouldn’t care about the amount of time we’re spending on scalar code per se, but the amount of time taken to sort the entire array.<br />
The decision to go to with scalar insertion-sort or stick to vectorized code is controlled by the threshold I mentioned before, which is still sitting there at <code class="highlighter-rouge">16</code>. We’re only beginning our optimization phase in the next post, so for now, we’ll stick with the threshold selected for <code class="highlighter-rouge">Array.Sort</code> by the CoreCLR developers, this is the “correct” starting point both in terms of allowing us to compare apples-to-apples and also as I am a firm believer at doing very incremental modifications for this sort of work.<br />
Having said that, this is definitely something we will tweak later for our particular implementation.</p>

<h2 id="finishing-off-with-a-sour-taste">Finishing off with a sour taste</h2>

<p>I’ll end this post with a not so easy pill to swallow: let’s re-run <code class="highlighter-rouge">perf</code> and measure a different aspect of our code: Let’s see how the code is behaving in terms of top-level performance counters. The idea here is to use counters that our CPU is already capable of collecting at the hardware level, with almost no performance impact, to see where/if we’re hurting. What I’ll do before invoking <code class="highlighter-rouge">perf</code> is use a Linux utility called <a href="https://github.com/lpechacek/cpuset"><code class="highlighter-rouge">cset</code></a> which can be <a href="https://stackoverflow.com/a/13076880/9172">used to</a> evacuate all user threads and (almost all) kernel threads from a given physical CPU core, using <a href="https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/cgroup-v1/cpusets.rst">cpusets</a>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="nv">$ </span><span class="nb">sudo </span>cset shield <span class="nt">--cpu</span> 3 <span class="nt">-k</span> on
cset: <span class="nt">--</span><span class="o">&gt;</span> activating shielding:
cset: moving 638 tasks from root into system cpuset...
<span class="o">[==================================================]</span>%
cset: kthread shield activated, moving 56 tasks into system cpuset...
<span class="o">[==================================================]</span>%
cset: <span class="k">**</span><span class="o">&gt;</span> 38 tasks are not movable, impossible to move
cset: <span class="s2">"system"</span> cpuset of CPUSPEC<span class="o">(</span>0-2<span class="o">)</span> with 667 tasks running
cset: <span class="s2">"user"</span> cpuset of CPUSPEC<span class="o">(</span>3<span class="o">)</span> with 0 tasks running
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Once we have “shielded” a single CPU core, we execute the <code class="highlighter-rouge">Example</code> binary we used before much in the same way while collecting different top-level hardware statistics from befre using a the following <code class="highlighter-rouge">perf</code> command line:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="rouge-code"><pre><span class="nv">$ </span>perf <span class="nb">stat</span> <span class="nt">-a</span> <span class="nt">--topdown</span> <span class="nb">sudo </span>cset shield <span class="nt">-e</span> ./Example <span class="se">\</span>
    <span class="nt">--type-list</span> DoublePumpedNaive <span class="nt">--size-list</span> 1000000
cset: <span class="nt">--</span><span class="o">&gt;</span> last message, executed args into cpuset <span class="s2">"/user"</span>, new pid is: 16107

 Performance counter stats <span class="k">for</span> <span class="s1">'system wide'</span>:
        retiring      bad speculation       frontend bound        backend bound
...
S0-C3 1    37.6%                32.3%                16.9%                13.2%

       3.221968791 seconds <span class="nb">time </span>elapsed

</pre></td></tr></tbody></table></code></pre></div></div>

<p>I’m purposely showing only the statistics collected for our shielded core since we know we only care about that core in the first place.</p>

<p>Here are some bad news: core #3 is really not having a good time running our code. <code class="highlighter-rouge">perf --topdown</code> is essentially screaming from the top of its lungs with that <code class="highlighter-rouge">32.3%</code> under the <code class="highlighter-rouge">bad speculation</code> column. This might seem like an innocent metric if you haven’t done this sort of thing before (in which case, read the info box below), but this is <strong>really bad</strong>. In plain English and <a href="https://easyperf.net/blog/2019/02/09/Top-Down-performance-analysis-methodology">without getting into the intricacies of top-down perfromance analysis</a>, this metric represents cycles where the CPU isn’t doing useful work because of an earlier mis-speculation. Here, the mis-speculation is mis-predicted branches. The penalty for <em>each</em> such mis-predicted branch is an entire flush of the pipeline (hence the wasted time), which costs us around 14-15 cycles on modern Intel CPUs.</p>

<table style="margin-bottom: 0em" class="notice--info">
<tr>
<td style="border: none; padding-top: 0; padding-bottom: 0; vertical-align: top"><span class="uk-label">Note</span></td>
<td style="border: none; padding-top: 0; padding-bottom: 0"><div>
        <p>We have to remember that efficient execution on modern CPUs means keeping the CPU pipeline as busy as possible; This is quite a challenge given its length is about 15 stages, and the CPU itself is super-scalar (For example: an <a href="https://en.wikichip.org/wiki/intel/microarchitectures/skylake_(client)#Individual_Core">Intel Skylake CPU has 8 ports</a> that can execute some instruction every cycle!). If, for example, all instructions in the CPU have a constant latency in cycles, this means it <em>has</em> to process 100+ instructions into “the future” while it’s just finishing up with a current one to avoid doing nothing. That’s enough of a challenge for regular code, but what should it do when it sees a branch? It could attempt and execute <strong>both</strong> branches, which quickly becomes a fool’s errand if somewhere close-by there would be even more branches. What CPU designers did was opt for speculative execution: add complex machinery to <em>predict</em> if a branch will be taken and speculatively execute the next instruction according to the prediction. But the predictor isn’t all knowing, and it will mis-predict, and then we end up paying a huge penalty: The CPU will have to push those mis-predicted instructions through the pipeline flushing the results out as if the whole thing never happenned. This is why the rate of mis-prediction is a life and death matter when it comes to performance.</p>
      </div>
</td>
</tr>
</table>

<p>Wait, I sense some optimistic thoughts all across the internet… maybe it’s not our precious vectorized so-called branch-less code? Maybe we can chalk it all up on that mean scalar <code class="highlighter-rouge">InsertionSort</code> function doing those millions and millions of scalar comparisons? We are, after all, using it for sorting small partitions, which we’ve already measured at more than 20% of the total run-time? Let’s see this again with <code class="highlighter-rouge">perf</code>, <em>this time</em> focusing on the <code class="highlighter-rouge">branch-misses</code> HW counter and try to figure out how the mis-predictions are distributed amongst our call-stacks:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre><span class="nv">$ </span><span class="nb">export </span><span class="nv">COMPlus_PerfMapEnabled</span><span class="o">=</span>1 <span class="c"># Make perf speak to the JIT</span>
<span class="c"># Record some performance information:</span>
<span class="nv">$ </span>perf record <span class="nt">-F</span> max <span class="nt">-e</span> branch-misses ./Example <span class="se">\</span>
    <span class="nt">--type-list</span> DoublePumpedNaive <span class="nt">--size-list</span> 1000000
...
<span class="nv">$ </span>perf report <span class="nt">--stdio</span> <span class="nt">-F</span> overhead,sym | <span class="nb">head</span> <span class="nt">-17</span>
...
    40.97%  <span class="o">[</span>.] ...::InsertionSort<span class="o">(!!</span>0<span class="k">*</span>,!!0<span class="k">*</span><span class="o">)[</span>Optimized]
    32.30%  <span class="o">[</span>.] ...::VectorizedPartitionInPlace<span class="o">(</span>int32<span class="k">*</span>,int32<span class="k">*</span>,int32<span class="k">*</span><span class="o">)[</span>Optimized]
     9.64%  <span class="o">[</span>.] ...::Memmove<span class="o">(</span>uint8&amp;,uint8&amp;,uint64<span class="o">)[</span>OptimizedTier1]
     9.64%  <span class="o">[</span>.] ...::QuickSortInt<span class="o">(</span>int32<span class="k">*</span>,int32<span class="k">*</span>,int32<span class="k">*</span>,int32<span class="k">*</span><span class="o">)[</span>OptimizedTier1]
     5.62%  <span class="o">[</span>.] ...::VectorizedPartitionOnStack<span class="o">(</span>int32<span class="k">*</span>,int32<span class="k">*</span>,int32<span class="k">*</span><span class="o">)[</span>Optimized]
...
</pre></td></tr></tbody></table></code></pre></div></div>

<p>No such luck. While <code class="highlighter-rouge">InsertionSort</code> is definitely starring here with 41% <em>of the</em> branch misprediction events, we still have <strong>32%</strong> of the bad speculation coming from our own new vectorized code. This is a red-flag as far as we’re concerned: It means that our vectorized code still contains a lot of mis-predicted branches. Given that we’re in the bussiness of sorting (random data) and the high rate of recorded mis-prediction the only logical conclusion is that we have branches that are data-dependent. Another thing to keep in mind is that the resulting pipeline flush is a large penalty to pay given that our entire 8-element partition block has a throughput of around 8-9 cycles. That means we are hitting that 15 cycle pan-to-the-face way too often to feel good about ourselves.</p>

<p>I’ll finish this post here. We have a <strong>lot of work</strong> cut out for us. This is no-where near over.<br />
In the next post, I’ll try to give the current vectorized code a good shakeup. After all, it’s still our biggest target in terms of number of instructions executed, and 2<sup>nd</sup> when it comes to branch mis-predictions. Once we finish squeezing that lemon for all its performance juice on the 4<sup>th</sup> post, We will turn our focus to the <code class="highlighter-rouge">InsertionSort</code> function on the 5<sup>th</sup> post , and we’ll see if we can appease the performance gods to make that part of the sorting effort faster.<br />
In the meantime, you can go back to the vectorized partitioning function and try to figure out what is causing all those nasty branch mis-predictions if you’re up for a small challenge. We’ll be dealing with it head-on in the next post.</p>

<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:0" role="doc-endnote">
      <p>For some, <code class="highlighter-rouge">perf</code> wasn’t in the mood to show me function names without calling <code class="highlighter-rouge">dotnet publish</code>  and using the resulting binary, and I didn’t care enough to investigate further… <a href="#fnref:0" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
